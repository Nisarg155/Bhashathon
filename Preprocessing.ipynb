{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### ✅ Standardize Format",
   "id": "f5eeadd2aaa8b8f7"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-25T05:58:57.869532Z",
     "start_time": "2025-02-25T05:58:53.772053Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File paths\n",
    "misspelled_file = \"artificial.train.src\"  # File containing incorrect sentences\n",
    "corrected_file = \"artificial.train.tgt\"   # File containing correct sentences\n",
    "output_csv = \"csv_files/standardized_dataset.csv\"  # Output CSV file\n",
    "\n",
    "# Read misspelled sentences\n",
    "with open(misspelled_file, \"r\", encoding=\"utf-8\") as file:\n",
    "    misspelled_sentences = [line.strip() for line in file.readlines() if line.strip()]  # Remove empty lines\n",
    "\n",
    "# Read corrected sentences\n",
    "with open(corrected_file, \"r\", encoding=\"utf-8\") as file:\n",
    "    corrected_sentences = [line.strip() for line in file.readlines() if line.strip()]  # Remove empty lines\n",
    "\n",
    "# Ensure both files have the same number of lines\n",
    "if len(misspelled_sentences) != len(corrected_sentences):\n",
    "    raise ValueError(\"Mismatch between number of misspelled and corrected sentences.\")\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\"Misspelled\": misspelled_sentences, \"Corrected\": corrected_sentences})\n",
    "\n",
    "# Remove duplicates (if any)\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(output_csv, index=False)\n",
    "\n",
    "print(f\"Standardized dataset saved to: {output_csv}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardized dataset saved to: csv_files/standardized_dataset.csv\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### ✅ Check for Unintended Errors",
   "id": "2a1ceed521ad0c40"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T18:30:37.679231Z",
     "start_time": "2025-02-18T18:30:14.175136Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import Levenshtein\n",
    "\n",
    "# File paths\n",
    "input_csv = \"csv_files/standardized_dataset.csv\"  # Input dataset\n",
    "output_csv = \"csv_files/filtered_dataset.csv\"  # Output filtered dataset\n",
    "removed_file = \"csv_files/removed_sentences.txt\"  # File to store removed sentences\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    text = str(text).lower().strip()  # Convert to lowercase and remove leading/trailing spaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # Remove extra spaces\n",
    "    return text\n",
    "\n",
    "# Load dataset without checking column names\n",
    "df = pd.read_csv(input_csv, header=0)  # Read with first row as column headers\n",
    "\n",
    "# Ensure there are at least two columns\n",
    "if df.shape[1] < 2:\n",
    "    raise ValueError(\"The input CSV must contain at least two columns (Incorrect, Corrected).\")\n",
    "\n",
    "# Automatically select the first two columns as Incorrect and Corrected\n",
    "df.iloc[:, 0] = df.iloc[:, 0].apply(clean_text)  # First column (Incorrect)\n",
    "df.iloc[:, 1] = df.iloc[:, 1].apply(clean_text)  # Second column (Corrected)\n",
    "\n",
    "# File to store removed sentences\n",
    "with open(removed_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"Removed Sentences:\\n\\n\")\n",
    "\n",
    "# Function to check if a word is unrealistic (Levenshtein distance > 4)\n",
    "def is_unrealistic_word(misspelled_word, correct_word, threshold=4):\n",
    "    return Levenshtein.distance(misspelled_word, correct_word) > threshold\n",
    "\n",
    "# Function to check if a sentence contains unrealistic words\n",
    "def contains_unrealistic_words(misspelled_sentence, corrected_sentence, threshold=4):\n",
    "    misspelled_words = misspelled_sentence.split()\n",
    "    corrected_words = corrected_sentence.split()\n",
    "\n",
    "    # If sentences have different word counts, return True\n",
    "    if len(misspelled_words) != len(corrected_words):\n",
    "        with open(removed_file, \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\"Removing (Word Count Mismatch):\\nMisspelled: {misspelled_sentence}\\nCorrected: {corrected_sentence}\\n\\n\")\n",
    "        return True  # Avoid cases where words are completely different\n",
    "\n",
    "    # Compare words one by one\n",
    "    for mw, cw in zip(misspelled_words, corrected_words):\n",
    "        if is_unrealistic_word(mw, cw, threshold):\n",
    "            with open(removed_file, \"a\", encoding=\"utf-8\") as f:\n",
    "                f.write(f\"Removing (Unrealistic Word Found):\\nMisspelled: {misspelled_sentence}\\nCorrected: {corrected_sentence}\\n\\n\")\n",
    "            return True  # Sentence contains an unrealistic word\n",
    "    return False\n",
    "\n",
    "# Apply filtering\n",
    "original_size = len(df)\n",
    "filtered_df = df[~df.apply(lambda row: contains_unrealistic_words(row.iloc[0], row.iloc[1], threshold=4), axis=1)]\n",
    "filtered_size = len(filtered_df)\n",
    "\n",
    "# Save filtered dataset\n",
    "filtered_df.to_csv(output_csv, index=False, header=False)  # Save without headers\n",
    "\n",
    "# Print summary log\n",
    "print(f\"\\nOriginal dataset size: {original_size}\")\n",
    "print(f\"Filtered dataset size: {filtered_size}\")\n",
    "print(f\"Number of sentences removed: {original_size - filtered_size}\")\n",
    "print(f\"Filtered dataset saved to: {output_csv}\")\n",
    "print(f\"Removed sentences stored in: {removed_file}\")\n"
   ],
   "id": "1589796bfb0609ed",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original dataset size: 627429\n",
      "Filtered dataset size: 582326\n",
      "Number of sentences removed: 45103\n",
      "Filtered dataset saved to: csv_files/filtered_dataset.csv\n",
      "Removed sentences stored in: csv_files/removed_sentences.txt\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T06:02:08.024781Z",
     "start_time": "2025-02-25T06:02:04.725400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# # File paths\n",
    "# misspelled_file = \"/content/drive/MyDrive/artificial.train.src\"  # File containing incorrect sentences\n",
    "# corrected_file = \"/content/drive/MyDrive/artificial.train.tgt\"   # File containing correct sentences\n",
    "# output_dir = \"/content/drive/MyDrive/sampled_datasets/\"  # Output directory for CSV files\n",
    "\n",
    "\n",
    "# File paths\n",
    "misspelled_file = \"artificial.train.src\"  # File containing incorrect sentences\n",
    "corrected_file = \"artificial.train.tgt\"   # File containing correct sentences\n",
    "output_dir = \"sampled_datasets/\"  # Output directory for CSV files\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Read misspelled sentences\n",
    "with open(misspelled_file, \"r\", encoding=\"utf-8\") as file:\n",
    "    misspelled_sentences = [line.strip() for line in file.readlines() if line.strip()]\n",
    "\n",
    "# Read corrected sentences\n",
    "with open(corrected_file, \"r\", encoding=\"utf-8\") as file:\n",
    "    corrected_sentences = [line.strip() for line in file.readlines() if line.strip()]\n",
    "\n",
    "# Ensure both files have the same number of lines\n",
    "if len(misspelled_sentences) != len(corrected_sentences):\n",
    "    raise ValueError(\"Mismatch between number of misspelled and corrected sentences.\")\n",
    "\n",
    "# Get total dataset size\n",
    "total_size = len(misspelled_sentences)\n",
    "chunk_size = 50000  # Maximum lines per CSV\n",
    "num_chunks = (total_size + chunk_size - 1) // chunk_size  # Calculate number of files needed\n",
    "\n",
    "# Split and save in chunks\n",
    "for i in range(num_chunks):\n",
    "    start_idx = i * chunk_size\n",
    "    end_idx = min((i + 1) * chunk_size, total_size)\n",
    "\n",
    "    # Extract chunk\n",
    "    chunk_misspelled = misspelled_sentences[start_idx:end_idx]\n",
    "    chunk_corrected = corrected_sentences[start_idx:end_idx]\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame({\"Misspelled\": chunk_misspelled, \"Corrected\": chunk_corrected})\n",
    "\n",
    "    # Remove duplicates (if any)\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    # Save to CSV\n",
    "    output_csv = os.path.join(output_dir, f\"sampled_dataset_part_{i+1}.csv\")\n",
    "    df.to_csv(output_csv, index=False)\n",
    "\n",
    "    print(f\"✅ Saved {output_csv} ({len(df)} rows)\")\n",
    "\n",
    "print(\"✅ All dataset parts saved successfully!\")\n"
   ],
   "id": "2334e19088d399f4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved sampled_datasets/sampled_dataset_part_1.csv (49845 rows)\n",
      "✅ Saved sampled_datasets/sampled_dataset_part_2.csv (49813 rows)\n",
      "✅ Saved sampled_datasets/sampled_dataset_part_3.csv (49827 rows)\n",
      "✅ Saved sampled_datasets/sampled_dataset_part_4.csv (49878 rows)\n",
      "✅ Saved sampled_datasets/sampled_dataset_part_5.csv (49875 rows)\n",
      "✅ Saved sampled_datasets/sampled_dataset_part_6.csv (49844 rows)\n",
      "✅ Saved sampled_datasets/sampled_dataset_part_7.csv (49783 rows)\n",
      "✅ Saved sampled_datasets/sampled_dataset_part_8.csv (49791 rows)\n",
      "✅ Saved sampled_datasets/sampled_dataset_part_9.csv (49753 rows)\n",
      "✅ Saved sampled_datasets/sampled_dataset_part_10.csv (49613 rows)\n",
      "✅ Saved sampled_datasets/sampled_dataset_part_11.csv (49857 rows)\n",
      "✅ Saved sampled_datasets/sampled_dataset_part_12.csv (49811 rows)\n",
      "✅ Saved sampled_datasets/sampled_dataset_part_13.csv (30972 rows)\n",
      "✅ All dataset parts saved successfully!\n"
     ]
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
